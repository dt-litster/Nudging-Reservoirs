\documentclass[12pt,oneside]{article}

\newcommand{\defEx}[2]{\textbf{#1} #2}

% This package simply sets the margins to be 1 inch.
\usepackage[margin=1in]{geometry}

% These packages include nice commands from AMS-LaTeX
\usepackage{amssymb,amsmath,amsthm, graphicx}

% Make the space between lines slightly more
% generous than normal single spacing, but compensate
% so that the spacing between rows of matrices still
% looks normal.  Note that 1.1=1/.9090909...
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\arraystretch}{.91}

\newcommand{\ang}[1]{\langle #1 \rangle}

% Define an environment for exercises.
\newenvironment{introduction}[1]{\vspace{.1in}\noindent\textbf{Introduction #1 \hspace{.05em}}}{}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\newenvironment{indentedexample}
  {\begin{example}\addtolength{\leftskip}{2em}}
  {\end{example}}

\DeclareMathOperator{\vsspan}{span}

%\NewDocumentCommand{\defEx}{ m m m O{0mm} O{6mm} }{
 %   \noindent\normalsize\textbf{#1}
  %  #2  
   % \vspace{#4}
    %\begin{indentedexample}
     %   #3
    %\end{indentedexample}
    %\vspace{#5}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{flushright}
\normalsize{D.T. Litster, Melanie Neller, Dallin Seyfried}  \\
Math 513R-Barker\\
\today
\end{flushright}

\begin{center}
\LARGE \textbf{Lecture Notes} \\
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\noindent\normalsize{Before we discuss the experiments that we ran we must first recall some data assimilation ideas.}\\


\noindent\large\textbf{Definitions:}\\
\noindent\normalsize{We will begin by discussing some important jargon, how it relates to our research, and examples of how they are used.}\\ 

% \noindent\normalsize\textbf{Congruence in the Integers:}
%     Let $a,b,n \in \mathbb{Z}$ with $n > 0$.  Then $a$ is congruent to $b$ modulo $n$ provided that $n | (a-b)$.  

% \indent\begin{example}
%     $[4]_{12} = \{a \in \mathbb{Z} \hspace{1mm} | \hspace{1mm} 12|(a-4)\} = \{\dots, -8, 4, 16, \dots \}$ is the set of integers congruent to $4$ modulo $12$.  $\mathbb{Z}_{12}$ is isomorphic to the musical clock used in our paper.
% \end{example}

\defEx{Nudging:}
{Nudging is a data assimilation technique that relaxes the model state toward observations by adding new terms, proportional to the difference between observations and model state, to the prognostic equations.}

\defEx{Reservoir Computing:}
{A reservoir computer is a machine learning model that can be used to predict the future states of time-dependent processes.  They are often used in modeling dynamical systems (such as the Lorenz attractor), pattern classification, and speech recognition.}

\pagebreak


\begin{center}
    \LARGE Notes on Nudging in Data Assimilation \\
\end{center}

\section{Nudging Introduction}
Nudging is a data assimilation technique that relaxes the model state toward observations by adding correction terms to the model's governing equations. These corrections are proportional to the difference between the observed data and the model state. Nudging is widely used in numerical weather prediction and other geophysical applications to improve the accuracy of forecasts.

\section{Theoretical Foundations}
\subsection{Definition of Nudging}
Nudging modifies the model's prognostic equations by introducing a term that "nudges" the model state \( \mathbf{x} \) toward observations \( \mathbf{y} \). The modified equation can be written as:


\[
\frac{d\mathbf{x}}{dt} = \mathbf{F}(\mathbf{x}, t) + \mathbf{G}(\mathbf{x}, \mathbf{y}),
\]


where:
\begin{itemize}
    \item \( \mathbf{F}(\mathbf{x}, t) \): The original model dynamics.
    \item \( \mathbf{G}(\mathbf{x}, \mathbf{y}) = \mathbf{K} (\mathbf{y} - \mathbf{x}) \): The nudging term.
    \item \( \mathbf{K} \): A gain matrix that determines the strength of the nudging.
\end{itemize}

\subsection{Key Assumptions}
Nudging assumes:
\begin{itemize}
    \item Observations \( \mathbf{y} \) are available at regular intervals.
    \item The model state \( \mathbf{x} \) is close to the true state of the system.
    \item The nudging term does not destabilize the model dynamics.
\end{itemize}

\subsection{Comparison with Other Data Assimilation Methods}
Unlike variational methods (e.g., 3D-Var, 4D-Var) or ensemble-based methods (e.g., Ensemble Kalman Filter), nudging is computationally simpler and does not require solving optimization problems or generating ensembles. However, it may be less accurate in systems with highly nonlinear dynamics.

\section{Applications of Nudging}
\subsection{Numerical Weather Prediction}
Nudging is commonly used in weather forecasting to incorporate observational data (e.g., temperature, wind speed) into atmospheric models. By continuously adjusting the model state toward observations, nudging helps maintain forecast accuracy over time.

\subsection{Oceanography}
In ocean modeling, nudging is used to assimilate data such as sea surface temperature and salinity. This improves the representation of ocean currents and other physical processes.

\subsection{Climate Modeling}
Nudging is applied in climate models to constrain simulations to observed historical data, enabling better analysis of long-term trends and variability.

\section{Mathematical Formulation}
The nudging term \( \mathbf{G}(\mathbf{x}, \mathbf{y}) \) is typically defined as:


\[
\mathbf{G}(\mathbf{x}, \mathbf{y}) = \mathbf{K} (\mathbf{y} - \mathbf{x}),
\]


where \( \mathbf{K} \) is the gain matrix. The choice of \( \mathbf{K} \) is critical:
\begin{itemize}
    \item A large \( \mathbf{K} \) results in rapid adjustment but may destabilize the model.
    \item A small \( \mathbf{K} \) ensures stability but may lead to slower convergence.
\end{itemize}

\section{Advantages and Limitations}
\subsection{Advantages}
\begin{itemize}
    \item Computationally efficient compared to variational and ensemble methods.
    \item Easy to implement in existing models.
    \item Effective for systems with frequent and reliable observations.
\end{itemize}

\subsection{Limitations}
\begin{itemize}
    \item May not perform well in highly nonlinear systems.
    \item Requires careful tuning of the gain matrix \( \mathbf{K} \).
    \item Assumes that observational errors are small and unbiased.
\end{itemize}

\section{Conclusion}
Nudging is a valuable tool in data assimilation, particularly for applications where computational efficiency is critical. While it has limitations compared to more sophisticated methods, its simplicity and effectiveness make it a popular choice in many fields.

\pagebreak


\begin{center}
    \LARGE Notes on Reservoir Computing \\
\end{center}

\section{Reservoir Computing Introduction}
A reservoir computer is a machine learning model that can be used to predict the future states of time-dependent processes such as chaotic dynamical systems.

\section{Theoretical Foundations}
\subsection{Reservoir Computing Definition}
A \textbf{reservoir} is a model for time series prediction where an input signal $u(t) \in \mathbb{R}^m$ over a time period $[0,T]$ drives a network $A \in \mathbb{R}^{n\times n}$ 
which reconstructs the signal to make a prediction $\hat{u}(t) \in \mathbb{R}^m$ in the future ($t > T$).  The Reservoir Computer has three stages of life:
\begin{enumerate}
    \item Processing
    \item Aggregation
    \item Prediction
\end{enumerate}

\subsection{Processing}
In the processing step, then odes of a network are driven by the input-signal $u(t)$ over the training interval $t \in [0,T]$.  In reservoir computing, this processing network
can be any network.  Typically, in our examples, the network is given by $A \in \mathbb{R}^{n\times n}$ where the entry $A_{ij} \in \mathbb{R}$ represents the weighted
connection from node $j$ to node $i$.

In the model, the processing network's nodes evolve according to the following differential equation:
\begin{align*}
    \frac{d}{dt}r(t) = \gamma [-r(t) + f(\rho A r(t) + \sigma W_{in}u(t))]
\end{align*}
for $t \in [0,T]$ where $r(t) \in \mathbb{R}^n$ represents the state of the nodes within the reservoir at time $t \in [0,T]$.  The matrix $W_{in} \in \mathbb{R}^{n\times m}$
in the differential equation is fixed and sends a linear combination of the $m-$dimensional training data $u(t) \in \mathbb{R}^m$ to each of the $n$ reservoir nodes.  
Typically $W_{in}$ is chosen from a uniform distribution $[W_{in}]_{ij} \sim U(-1,1)$.  The function $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$ in the above equation is applied
element wise and $\gamma, \rho, \sigma$ are nonnegative scalar parameters of the reservoir.

As the input-signal $u(t)$ appears in the given system, the nodes of the reservoir's processing network depend on, or are driven by, this time-series.  The nodes' response
to this input signal is given by the solution $r(t)$ to this differential equation over the training period $t \in [0,T]$.  Matching what is typically done in the field,
the initial reservoir state $r(0) = r_0$ is typically initialized randomly, with each coordinate drawn from a uniform distribution.

\subsection{Aggregation}
In the second step of the life cycle we aggregate the network responses.  Using a discretized time system we compute the matrix:
\begin{align*}
    W_{out} = \text{argmin}_{W \in \mathbb{R}^{m \times n}} \left [ \sum_{i=0}^l ||Wr(t_i) - u(t_i)||_2^2 + \alpha ||W||_2^2 \right ]
\end{align*}
that minimizes the sum on the right.  Here, the $\alpha$ parameter can be thought of as a Ridge Regression term to reduce overfitting.
The result is the mapping $W_{out} \in \mathbb{R}^{m \times n}$, which is used to aggregate the network responses into the vector $\hat{u}(t) = W_{out}r(t) \in \mathbb{R}^m$.
This can, roughly speaking, be considered to be a proxy for the input signal, i.e. $\hat{u}(t) \approx u(t)$ over the training period $t \in [0,T]$.

\subsection{Prediction}
The last step is done by replacing the input-signal $u(t)$ by the aggregate response vector $\hat{u}(t)$ in the differential system:
\begin{align*}
    \frac{d}{dt}\hat{r}(t) = \gamma[-\hat{r}(t) + f([\rho A + \sigma W_{in}W_{out}]\hat{r}(t))]
\end{align*}
which no longer depends on the training data.  Because of this, we refer to this system as the trained reservoir and use the valid prediction time metric to calculate how well 
the prediction does against the actual signal.

\subsection{Valid Prediction Time Definition}
Let $d(\cdot, \cdot)$ be a metric and let $\epsilon > 0$ be a tolerance.  The valid prediction time (VPT) of a prediction time-series $\hat{u}(t)$ beginning at time $t = T$
associated with the signal $u(t)$ is the largest value $t_* = T_* - T \geq 0$ such that the error:
\[d(u(t),\hat{u}(t)) \leq \epsilon\]
for all $t \in [T,T_*]$.

\subsection{Applications of Reservoir Computing}
Some common applications of Reservoir Computing include:
\begin{enumerate}
    \item Robotic Controllers
    \item Pattern Classification
    \item Attractor Reconstruction
    \item Forecasting Chaotic Systems
\end{enumerate}



\end{document}

